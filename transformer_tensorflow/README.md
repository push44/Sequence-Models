## Project Objective:

<ul>
    <li>Create positional encodings to capture sequential relationships in data.</li>
    <li>Calculate scaled dot-product self-attention with word embeddings.</li>
    <li>Implement masked multi-head attention</li>
    <li>Build and train a Transformer model</li>
</ul>